log_dir: "logs/CartPole/a2c/exp_0/"
load_checkpoint: null

env_type: &env_type "gym"
env_name: &env_name "CartPole-v1"
env_args: &env_args {}
observation_size: &observation_size 4
hidden_size: &hidden_size 16
action_size: &action_size 2

device_online: cpu
device_train: cpu

train_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args
  env_num: 4

test_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args
  env_num: 4

policy: &policy Categorical
policy_args: {}

actor_critic_nn_type: MLP
actor_critic_nn_args:
  observation_size: *observation_size
  hidden_size: *hidden_size
  action_size: *action_size
  distribution: *policy

train_agent_args:
  agent_type: A2C
  normalize_advantage: False
  returns_estimator: gae

  optimization_params:
    learning_rate: 0.001
    gamma: 0.99
    entropy: 0.0
    clip_grad: 0.5
    gae_lambda: 0.9

  additional_params: {}  # A2C have no additional parameters

trainer_args:
  update_period: 1

  normalize_obs: False
  train_obs_normalizer: False
  obs_clip: 10.0

  normalize_reward: False
  scale_reward: False
  train_reward_normalizer: False
  reward_clip: 10.0

training_args:
  n_epoch: 20
  n_steps_per_epoch: 100
  rollout_len: 16
  n_tests_per_epoch: 100
