log_dir: "logs/Mario/ppo_2/"
load_checkpoint: null

env_type: &env_type "retro"
env_name: &env_name "SuperMarioBros-Nes"
env_args: &env_args {}
action_size: &action_size 9
custom_wrapper_path: &wrapper_path "custom_wrappers.mario_wrapper:MarioWrapper"
custom_wrapper_args: &wrapper_args {}

device_online: cuda
device_train: cuda

train_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args

  action_repeat: 4
  image_args:
    x_size: 96
    y_size: 96

  custom_wrapper_path: *wrapper_path
  custom_wrapper_args: *wrapper_args

  env_num: 8

test_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args

  action_repeat: 4
  image_args:
    x_size: 96
    y_size: 96

  custom_wrapper_path: *wrapper_path
  custom_wrapper_args: *wrapper_args

  env_num: 2

policy: &policy Bernoulli
policy_args: {}

actor_critic_nn_type: DeepCNN
actor_critic_nn_args:
  action_size: *action_size
  distribution: *policy

train_agent_args:
  agent_type: PPO
  normalize_advantage: False
  returns_estimator: gae

  optimization_params:
    learning_rate: 0.0003
    gamma: 0.99
    entropy: 0.001
    clip_grad: 0.5
    gae_lambda: 0.95

  additional_params:  # special parameters for PPO
    rollback_alpha: 0.1
    ppo_n_epoch: 5
    ppo_n_mini_batches: 4

trainer_args:
  update_period: 1

  normalize_obs: False
  train_obs_normalizer: False
  obs_clip: 10.0

  normalize_reward: True
  scale_reward: False
  train_reward_normalizer: True
  reward_clip: 10.0

training_args:
  n_epoch: 10
  n_steps_per_epoch: 1000
  rollout_len: 128
  n_tests_per_epoch: 1  # environment is deterministic
