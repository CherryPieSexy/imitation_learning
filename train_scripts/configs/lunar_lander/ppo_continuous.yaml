log_dir: "logs/lunar_lander/ppo_continuous/"
load_checkpoint: null

env_type: &env_type "gym"
env_name: &env_name "LunarLanderContinuous-v2"
env_args: &env_args {}
observation_size: &observation_size 8
hidden_size: &hidden_size 64
action_size: &action_size 2

device_online: cpu
device_train: cpu

train_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args
  env_num: 8

test_env_args:
  env_type: *env_type
  env_name: *env_name
  env_args: *env_args
  env_num: 4

policy: &policy "Beta"
policy_args: {}

actor_critic_nn_type: "MLP"
actor_critic_nn_args:
  observation_size: *observation_size
  hidden_size: *hidden_size
  action_size: *action_size
  distribution: *policy

train_agent_args:
  agent_type: "PPO"
  normalize_advantage: False
  returns_estimator: "gae"

  optimization_params:
    learning_rate: 0.0003
    gamma: 0.99
    entropy: 0.0
    clip_grad: 0.5

  additional_params:  # special parameters for PPO
    ppo_epsilon: 0.1
    rollback_alpha: 0.0
    ppo_n_epoch: 8
    ppo_n_mini_batches: 8

trainer_args:
  update_period: 1

  normalize_obs: True
  train_obs_normalizer: True
  obs_clip: 10.0

  normalize_reward: False
  scale_reward: False
  train_reward_normalizer: False
  reward_clip: 10.0

training_args:
  n_epoch: 10
  n_steps_per_epoch: 500
  rollout_len: 64
  n_tests_per_epoch: 50
